#!/usr/bin/env python3
"""
AI GitHub Daily Tracker - AIé¡¹ç›®æ¯æ—¥è¿½è¸ªå™¨
æ¯å¤©è‡ªåŠ¨å‘ç°å¹¶æ¨é€æœ€å€¼å¾—å…³æ³¨çš„AIå¼€æºé¡¹ç›®
"""

import os
import json
import time
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import requests
from dateutil import parser as date_parser

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼ˆç”¨äºæœ¬åœ°æµ‹è¯•ï¼‰
try:
    from load_env import load_env
    load_env()
except ImportError:
    pass


class GitHubAPIClient:
    """GitHub APIå®¢æˆ·ç«¯ï¼Œè´Ÿè´£è·å–é¡¹ç›®æ•°æ®"""

    def __init__(self, token: Optional[str] = None):
        self.token = token or os.getenv('GH_TOKEN')
        self.base_url = 'https://api.github.com'
        self.session = requests.Session()
        if self.token:
            self.session.headers.update({'Authorization': f'token {self.token}'})

        self.logger = logging.getLogger(__name__)

    def search_repositories(self, query: str, sort: str = 'stars', order: str = 'desc', per_page: int = 50) -> List[Dict]:
        """æœç´¢GitHubä»“åº“"""
        url = f'{self.base_url}/search/repositories'
        params = {
            'q': query,
            'sort': sort,
            'order': order,
            'per_page': per_page
        }

        try:
            response = self.session.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            self.logger.info(f"GitHub APIè¯·æ±‚æˆåŠŸï¼Œæ‰¾åˆ° {data.get('total_count', 0)} ä¸ªé¡¹ç›®")
            return data.get('items', [])
        except requests.exceptions.RequestException as e:
            self.logger.error(f"GitHub APIè¯·æ±‚å¤±è´¥: {e}")
            return []

    def get_popular_ai_projects(self) -> List[Dict]:
        """è·å–æœ€å—æ¬¢è¿çš„AIé¡¹ç›®ï¼ˆæŒ‰staræ•°æ’åºï¼‰"""
        # åˆ†å¤šæ¬¡æŸ¥è¯¢ä¸åŒçš„AIé¢†åŸŸï¼Œé¿å…æŸ¥è¯¢è¿‡é•¿
        queries = [
            'machine learning OR "artificial intelligence" OR "deep learning"',
            'tensorflow OR pytorch OR keras',
            'nlp OR "natural language processing" OR transformer OR gpt',
            '"computer vision" OR opencv OR yolo'
        ]

        all_projects = []
        for query in queries:
            projects = self.search_repositories(query, sort='stars', per_page=25)
            all_projects.extend(projects)
            time.sleep(1)  # é¿å…APIé™åˆ¶

        # å»é™¤é‡å¤é¡¹ç›®ï¼ˆåŸºäºIDï¼‰
        seen_ids = set()
        unique_projects = []
        for project in all_projects:
            if project['id'] not in seen_ids:
                seen_ids.add(project['id'])
                unique_projects.append(project)

        return unique_projects

    def get_trending_ai_projects(self) -> List[Dict]:
        """è·å–è¶‹åŠ¿AIé¡¹ç›®ï¼ˆæŒ‰æ›´æ–°æ—¶é—´æ’åºï¼‰"""
        # è·å–æœ€è¿‘30å¤©å†…æ›´æ–°çš„é¡¹ç›®
        date_30_days_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')

        # åˆ†å¤šæ¬¡æŸ¥è¯¢ä¸åŒçš„AIé¢†åŸŸ
        queries = [
            f'machine learning pushed:>{date_30_days_ago}',
            f'tensorflow OR pytorch pushed:>{date_30_days_ago}',
            f'nlp OR gpt pushed:>{date_30_days_ago}',
            f'"computer vision" pushed:>{date_30_days_ago}'
        ]

        all_projects = []
        for query in queries:
            projects = self.search_repositories(query, sort='updated', per_page=25)
            all_projects.extend(projects)
            time.sleep(1)  # é¿å…APIé™åˆ¶

        # å»é™¤é‡å¤é¡¹ç›®ï¼ˆåŸºäºIDï¼‰
        seen_ids = set()
        unique_projects = []
        for project in all_projects:
            if project['id'] not in seen_ids:
                seen_ids.add(project['id'])
                unique_projects.append(project)

        return unique_projects


class AIProjectFilter:
    """AIé¡¹ç›®è¯†åˆ«å’Œè¿‡æ»¤å™¨"""

    AI_KEYWORDS = [
        'artificial intelligence', 'machine learning', 'deep learning', 'neural network',
        'computer vision', 'nlp', 'natural language processing', 'chatbot', 'tensorflow',
        'pytorch', 'llm', 'gpt', 'transformer', 'reinforcement learning', 'data science',
        'ai', 'openai', 'llama', 'bert', 'stable diffusion', 'generative ai', 'langchain',
        'keras', 'scikit-learn', 'pandas', 'numpy', 'opencv', 'yolo', 'cnn', 'rnn',
        'lstm', 'gan', 'vae', 'autoencoder', 'embedding', 'classification', 'regression',
        'clustering', 'recommendation', 'speech recognition', 'text mining', 'sentiment analysis'
    ]

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def is_ai_project(self, repo: Dict) -> bool:
        """åˆ¤æ–­é¡¹ç›®æ˜¯å¦ä¸ºAIç›¸å…³é¡¹ç›®"""
        # å®‰å…¨åœ°å¤„ç†å¯èƒ½ä¸ºNoneçš„å­—æ®µ
        name = repo.get('name') or ''
        description = repo.get('description') or ''
        topics = repo.get('topics') or []

        text_fields = [
            name.lower(),
            description.lower(),
            ' '.join(topics).lower()
        ]

        full_text = ' '.join(text_fields)

        for keyword in self.AI_KEYWORDS:
            if keyword in full_text:
                return True

        return False

    def filter_ai_projects(self, projects: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤å‡ºAIç›¸å…³é¡¹ç›®"""
        ai_projects = []
        for project in projects:
            if self.is_ai_project(project):
                ai_projects.append(project)

        self.logger.info(f"ä» {len(projects)} ä¸ªé¡¹ç›®ä¸­ç­›é€‰å‡º {len(ai_projects)} ä¸ªAIé¡¹ç›®")
        return ai_projects


class CommercialAIProjectFilter:
    """å•†ç”¨å®ç”¨æ€§AIé¡¹ç›®è¯†åˆ«å’Œè¿‡æ»¤å™¨"""

    COMMERCIAL_KEYWORDS = [
        # è‡ªåŠ¨åŒ–å·¥å…·å’Œå¹³å°
        'automation', 'workflow', 'n8n', 'zapier', 'automate', 'pipeline', 'scheduler',
        'cronjob', 'webhook', 'api automation', 'workflow automation',

        # ç¤¾äº¤åª’ä½“å’Œå†…å®¹ç®¡ç†
        'social media', 'instagram', 'twitter', 'linkedin', 'youtube', 'tiktok', 'facebook',
        'xiaohongshu', 'weibo', 'content management', 'social automation', 'post scheduler',
        'social media management', 'content generator', 'social bot', 'mcp',

        # çˆ¬è™«å’Œæ•°æ®é‡‡é›†
        'scraper', 'crawler', 'scraping', 'data extraction', 'web scraping', 'reddit crawler',
        'news crawler', 'price monitor', 'data collection', 'web automation', 'selenium',
        'beautifulsoup', 'scrapy',

        # ç”µå•†å’Œä¸šåŠ¡å·¥å…·
        'ecommerce', 'shopify', 'amazon', 'product management', 'inventory', 'price tracking',
        'dropshipping', 'affiliate', 'marketing automation', 'email marketing', 'crm',
        'lead generation', 'sales automation',

        # åŠå…¬å’Œç”Ÿäº§åŠ›
        'productivity', 'office automation', 'document processing', 'pdf automation',
        'excel automation', 'report generation', 'business intelligence', 'dashboard',
        'analytics', 'metrics', 'kpi', 'monitoring',

        # é€šä¿¡å’Œå®¢æœ
        'chatbot', 'customer service', 'support bot', 'telegram bot', 'discord bot',
        'slack bot', 'whatsapp', 'wechat', 'messaging', 'notification', 'alert',

        # å†…å®¹åˆ›ä½œå’Œåª’ä½“
        'content creation', 'blog automation', 'seo', 'keyword research', 'content generator',
        'video automation', 'image processing', 'thumbnail generator', 'media converter',

        # é‡‘èå’Œäº¤æ˜“
        'trading', 'crypto', 'stock', 'financial', 'investment', 'portfolio', 'market data',
        'price alert', 'trading bot', 'arbitrage',

        # å®ç”¨å·¥å…·
        'utility', 'tool', 'helper', 'assistant', 'generator', 'converter', 'validator',
        'formatter', 'calculator', 'manager', 'organizer', 'tracker'
    ]

    BUSINESS_INDICATORS = [
        # å•†ä¸šæ¨¡å¼ç›¸å…³
        'saas', 'api', 'service', 'platform', 'solution', 'enterprise', 'business',
        'commercial', 'professional', 'premium', 'subscription', 'freemium',

        # å®ç”¨æ€§æŒ‡æ ‡
        'ready to use', 'production ready', 'plug and play', 'easy setup', 'one click',
        'no code', 'low code', 'drag and drop', 'user friendly', 'gui', 'interface',

        # éƒ¨ç½²å’Œé›†æˆ
        'docker', 'kubernetes', 'cloud', 'aws', 'azure', 'google cloud', 'deployment',
        'hosting', 'server', 'microservice', 'rest api', 'graphql'
    ]

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def is_commercial_ai_project(self, repo: Dict) -> bool:
        """åˆ¤æ–­é¡¹ç›®æ˜¯å¦ä¸ºå•†ç”¨å®ç”¨æ€§AIé¡¹ç›®"""
        # é¦–å…ˆå¿…é¡»æ˜¯AIé¡¹ç›®
        ai_filter = AIProjectFilter()
        if not ai_filter.is_ai_project(repo):
            return False

        # å®‰å…¨åœ°å¤„ç†å¯èƒ½ä¸ºNoneçš„å­—æ®µ
        name = repo.get('name') or ''
        description = repo.get('description') or ''
        topics = repo.get('topics') or []

        # é¢å¤–è€ƒè™‘readmeå†…å®¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        text_fields = [
            name.lower(),
            description.lower(),
            ' '.join(topics).lower()
        ]

        full_text = ' '.join(text_fields)

        # æ£€æŸ¥å•†ç”¨å…³é”®è¯
        commercial_score = 0
        for keyword in self.COMMERCIAL_KEYWORDS:
            if keyword in full_text:
                commercial_score += 2

        # æ£€æŸ¥å•†ä¸šæŒ‡æ ‡å…³é”®è¯
        for keyword in self.BUSINESS_INDICATORS:
            if keyword in full_text:
                commercial_score += 1

        # é¢å¤–çš„å•†ç”¨æ€§è¯„ä¼°
        stars = repo.get('stargazers_count', 0)
        forks = repo.get('forks_count', 0)

        # é«˜staræ•°çš„å®ç”¨å·¥å…·æ›´å¯èƒ½æ˜¯å•†ç”¨é¡¹ç›®
        if stars > 1000:
            commercial_score += 1
        if stars > 5000:
            commercial_score += 1

        # é«˜forkç‡é€šå¸¸è¡¨ç¤ºå®ç”¨æ€§
        if forks > 0 and stars > 0:
            fork_ratio = forks / stars
            if fork_ratio > 0.1:  # 10%ä»¥ä¸Šçš„forkç‡
                commercial_score += 2

        # å•†ç”¨æ€§è¯„åˆ†é˜ˆå€¼
        return commercial_score >= 3

    def filter_commercial_ai_projects(self, projects: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤å‡ºå•†ç”¨å®ç”¨æ€§AIé¡¹ç›®"""
        commercial_projects = []
        for project in projects:
            if self.is_commercial_ai_project(project):
                commercial_projects.append(project)

        self.logger.info(f"ä» {len(projects)} ä¸ªé¡¹ç›®ä¸­ç­›é€‰å‡º {len(commercial_projects)} ä¸ªå•†ç”¨å®ç”¨æ€§AIé¡¹ç›®")
        return commercial_projects


class ProjectDeduplicator:
    """é¡¹ç›®å»é‡å™¨ï¼Œç®¡ç†å·²æ¨é€é¡¹ç›®è®°å½•"""

    def __init__(self, storage_file: str = 'sent_projects.json'):
        self.storage_file = storage_file
        self.logger = logging.getLogger(__name__)
        self.sent_projects = self._load_sent_projects()

    def _load_sent_projects(self) -> Dict:
        """åŠ è½½å·²æ¨é€é¡¹ç›®è®°å½•"""
        if os.path.exists(self.storage_file):
            try:
                with open(self.storage_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                self.logger.error(f"è¯»å–å·²æ¨é€é¡¹ç›®è®°å½•å¤±è´¥: {e}")
                return {}
        return {}

    def _save_sent_projects(self):
        """ä¿å­˜å·²æ¨é€é¡¹ç›®è®°å½•"""
        try:
            with open(self.storage_file, 'w', encoding='utf-8') as f:
                json.dump(self.sent_projects, f, ensure_ascii=False, indent=2)
        except IOError as e:
            self.logger.error(f"ä¿å­˜å·²æ¨é€é¡¹ç›®è®°å½•å¤±è´¥: {e}")

    def is_project_sent(self, repo_id: str) -> bool:
        """æ£€æŸ¥é¡¹ç›®æ˜¯å¦å·²æ¨é€"""
        return str(repo_id) in self.sent_projects

    def mark_project_as_sent(self, repo: Dict):
        """æ ‡è®°é¡¹ç›®ä¸ºå·²æ¨é€"""
        repo_id = str(repo['id'])
        self.sent_projects[repo_id] = {
            'name': repo['name'],
            'full_name': repo['full_name'],
            'sent_date': datetime.now().isoformat(),
            'stars': repo['stargazers_count'],
            'url': repo['html_url']
        }
        self._save_sent_projects()

    def clean_old_records(self, days: int = 30):
        """æ¸…ç†æ—§è®°å½•"""
        cutoff_date = datetime.now() - timedelta(days=days)
        to_remove = []

        for repo_id, project_info in self.sent_projects.items():
            try:
                sent_date = date_parser.parse(project_info['sent_date'])
                if sent_date < cutoff_date:
                    to_remove.append(repo_id)
            except (ValueError, TypeError):
                to_remove.append(repo_id)

        for repo_id in to_remove:
            del self.sent_projects[repo_id]

        if to_remove:
            self._save_sent_projects()
            self.logger.info(f"æ¸…ç†äº† {len(to_remove)} æ¡æ—§è®°å½•")

    def filter_new_projects(self, projects: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤å‡ºæœªæ¨é€çš„é¡¹ç›®"""
        new_projects = []
        for project in projects:
            if not self.is_project_sent(project['id']):
                new_projects.append(project)

        self.logger.info(f"ä» {len(projects)} ä¸ªé¡¹ç›®ä¸­è¿‡æ»¤å‡º {len(new_projects)} ä¸ªæœªæ¨é€é¡¹ç›®")
        return new_projects

    def reset_sent_projects(self):
        """é‡ç½®å·²æ¨é€é¡¹ç›®è®°å½•ï¼Œæ¸…ç©ºæ‰€æœ‰è®°å½•"""
        self.sent_projects = {}
        self._save_sent_projects()
        self.logger.info("å·²é‡ç½®æ‰€æœ‰æ¨é€è®°å½•ï¼Œä¸‹æ¬¡å°†æ¨é€æœ€çƒ­é—¨çš„é¡¹ç›®")

    def get_stats(self) -> Dict:
        """è·å–æ¨é€ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_sent': len(self.sent_projects),
            'latest_sent': max([proj['sent_date'] for proj in self.sent_projects.values()]) if self.sent_projects else None,
            'storage_file': self.storage_file
        }


class ProjectSummarizer:
    """é¡¹ç›®æ€»ç»“ç”Ÿæˆå™¨ï¼Œç”Ÿæˆæœ‰è¯´æœåŠ›çš„é¡¹ç›®ä»‹ç»"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def generate_summary(self, repo: Dict) -> str:
        """ç”Ÿæˆé¡¹ç›®çš„æ™ºèƒ½æ€»ç»“"""
        try:
            name = repo.get('name', '')
            description = repo.get('description', '')
            stars = repo.get('stargazers_count', 0)
            forks = repo.get('forks_count', 0)
            language = repo.get('language', 'Unknown')
            topics = repo.get('topics', [])
            created_at = repo.get('created_at', '')
            updated_at = repo.get('updated_at', '')

            # è®¡ç®—é¡¹ç›®å¹´é¾„
            try:
                created_date = date_parser.parse(created_at)
                days_since_creation = max((datetime.now().replace(tzinfo=None) -
                                         created_date.replace(tzinfo=None)).days, 1)
                years = days_since_creation / 365.25
            except:
                years = 0

            # åªä½¿ç”¨æŠ€æœ¯äº®ç‚¹å’Œå†…å®¹æè¿°ï¼Œé¿å…æ•°å­—é‡å¤
            technical_highlights = self._generate_technical_highlights(name, description, language, topics)

            if technical_highlights:
                return technical_highlights
            else:
                # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°æŠ€æœ¯äº®ç‚¹ï¼Œä½¿ç”¨é¡¹ç›®æè¿°
                return description[:80] + "..." if len(description) > 80 else description

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆé¡¹ç›®æ€»ç»“å¤±è´¥: {e}")
            return repo.get('description', 'æš‚æ— æè¿°')[:100]

    def _generate_popularity_reason(self, stars: int, forks: int, years: float) -> str:
        """ç”Ÿæˆå—æ¬¢è¿åŸå› """
        if stars >= 100000:
            return f"ä½œä¸ºGitHubä¸Šæœ€å—æ¬¢è¿çš„é¡¹ç›®ä¹‹ä¸€ï¼Œ{stars:,}ä¸ªstarè¯æ˜äº†å…¶åœ¨å¼€å‘è€…ç¤¾åŒºä¸­çš„ç»Ÿæ²»åœ°ä½ã€‚"
        elif stars >= 50000:
            return f"å‡­å€Ÿ{stars:,}ä¸ªstarå’Œ{forks:,}ä¸ªforkï¼Œå·²æˆä¸ºè¯¥é¢†åŸŸçš„æ ‡æ†é¡¹ç›®ã€‚"
        elif stars >= 20000:
            return f"è·å¾—{stars:,}ä¸ªstarï¼Œåœ¨å¼€å‘è€…ä¸­äº«æœ‰å¾ˆé«˜å£°èª‰ã€‚"
        elif stars >= 5000:
            if years < 1:
                return f"è™½ç„¶åˆ›å»ºä¸åˆ°ä¸€å¹´ï¼Œä½†å·²è·å¾—{stars:,}ä¸ªstarï¼Œå±•ç°å‡ºå¼ºåŠ²çš„å¢é•¿åŠ¿å¤´ã€‚"
            else:
                return f"æŒç»­è·å¾—ç¤¾åŒºè®¤å¯ï¼Œ{stars:,}ä¸ªstarä½“ç°äº†å…¶å®ç”¨ä»·å€¼ã€‚"
        elif stars >= 1000:
            if years < 0.5:
                return f"ä½œä¸ºæ–°å…´é¡¹ç›®ï¼Œ{stars:,}ä¸ªstaræ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚"
            else:
                return f"{stars:,}ä¸ªstarè¡¨æ˜å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„å½±å“åŠ›ã€‚"
        else:
            return f"æ­£åœ¨å¿«é€Ÿå‘å±•ä¸­çš„é¡¹ç›®ï¼Œå·²è·å¾—{stars}ä¸ªstarã€‚"

    def _generate_technical_highlights(self, name: str, description: str, language: str, topics: List[str]) -> str:
        """ç”ŸæˆæŠ€æœ¯äº®ç‚¹"""
        name_lower = name.lower()
        desc_lower = description.lower()
        topics_str = ' '.join(topics).lower()
        text = f"{name_lower} {desc_lower} {topics_str}"

        # TensorFlowç”Ÿæ€
        if 'tensorflow' in text:
            return "ç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ å¹³å°ï¼Œæ”¯æŒè®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´å·¥ä½œæµ"

        # PyTorchç”Ÿæ€
        elif 'pytorch' in text:
            return "åŠ¨æ€å›¾æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæä¾›çµæ´»çš„æ¨¡å‹æ„å»ºå’Œè°ƒè¯•èƒ½åŠ›"

        # Transformerså’ŒHugging Face
        elif 'transformers' in text or 'huggingface' in text or 'ğŸ¤—' in text:
            return "é¢„è®­ç»ƒæ¨¡å‹åº“ï¼ŒåŒ…å«BERTã€GPTç­‰æœ€æ–°NLPæ¨¡å‹"

        # å¤§è¯­è¨€æ¨¡å‹æ¨ç†
        elif any(keyword in text for keyword in ['llm', 'inference', 'vllm', 'serving']):
            return "é«˜æ€§èƒ½LLMæ¨ç†å¼•æ“ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæ¨ç†é€Ÿåº¦"

        # GPTå’Œå¯¹è¯AI
        elif any(keyword in text for keyword in ['gpt', 'chatgpt', 'chat', 'conversation']):
            return "å¯¹è¯AIå·¥å…·é›†ï¼Œæä¾›é«˜è´¨é‡çš„äººæœºäº¤äº’ä½“éªŒ"

        # è®¡ç®—æœºè§†è§‰
        elif any(keyword in text for keyword in ['opencv', 'yolo', 'detection', 'vision']):
            return "è®¡ç®—æœºè§†è§‰å·¥å…·åŒ…ï¼Œæ”¯æŒå›¾åƒå¤„ç†å’Œç›®æ ‡æ£€æµ‹"

        # Scikit-learn
        elif 'scikit' in text or 'sklearn' in text:
            return "ç»å…¸æœºå™¨å­¦ä¹ åº“ï¼Œæä¾›åˆ†ç±»ã€å›å½’ã€èšç±»ç­‰ç®—æ³•"

        # æ•°æ®ç§‘å­¦
        elif any(keyword in text for keyword in ['pandas', 'numpy', 'jupyter', 'data']):
            return "æ•°æ®åˆ†æå·¥å…·é“¾ï¼Œç®€åŒ–æ•°æ®å¤„ç†å’Œå¯è§†åŒ–æµç¨‹"

        # AIå­¦ä¹ èµ„æº
        elif any(keyword in text for keyword in ['tutorial', 'learning', 'course', 'beginner']):
            return "AIå­¦ä¹ æ•™ç¨‹ï¼Œä»åŸºç¡€æ¦‚å¿µåˆ°å®è·µé¡¹ç›®çš„å®Œæ•´æŒ‡å—"

        # è‡ªåŠ¨åŒ–å’Œå·¥å…·
        elif any(keyword in text for keyword in ['automation', 'tool', 'assistant', 'productivity']):
            return "AIè‡ªåŠ¨åŒ–å·¥å…·ï¼Œæå‡å¼€å‘æ•ˆç‡å’Œå·¥ä½œæµä¼˜åŒ–"

        # Awesomeç³»åˆ—
        elif 'awesome' in text:
            return "ç²¾é€‰èµ„æºåˆé›†ï¼Œæ±‡æ€»è¯¥é¢†åŸŸæœ€ä½³å®è·µå’Œå·¥å…·"

        # é€šç”¨æè¿°åŸºäºé¡¹ç›®æè¿°
        else:
            # ä»æè¿°ä¸­æå–å…³é”®ä¿¡æ¯
            if len(description) > 10:
                return description[:70] + "..." if len(description) > 70 else description
            else:
                return f"{language}å¼€å‘çš„AIå·¥å…·ï¼Œä¸“æ³¨è§£å†³å®é™…é—®é¢˜"

    def _generate_community_impact(self, stars: int, forks: int, years: float) -> str:
        """ç”Ÿæˆç¤¾åŒºå½±å“æè¿°"""
        fork_ratio = forks / max(stars, 1)

        if fork_ratio > 0.3:
            return "æ´»è·ƒçš„è´¡çŒ®è€…ç¤¾åŒºå’Œä¸°å¯Œçš„forkæ•°é‡æ˜¾ç¤ºäº†é¡¹ç›®çš„å¥åº·å‘å±•æ€åŠ¿ã€‚"
        elif fork_ratio > 0.15:
            return "è‰¯å¥½çš„ç¤¾åŒºå‚ä¸åº¦è¡¨æ˜é¡¹ç›®å…·æœ‰æŒç»­çš„å‘å±•åŠ¨åŠ›ã€‚"
        elif stars > 1000:
            return "è™½ç„¶forkè¾ƒå°‘ï¼Œä½†é«˜staræ•°æ˜¾ç¤ºäº†é¡¹ç›®çš„é«˜è´¨é‡å’Œå®ç”¨æ€§ã€‚"
        else:
            return "æ­£åœ¨å»ºç«‹ç¤¾åŒºåŸºç¡€ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å‘å±•å‰æ™¯ã€‚"


class TrendAnalyzer:
    """è¶‹åŠ¿åˆ†æå™¨ï¼Œè®¡ç®—é¡¹ç›®è¶‹åŠ¿åˆ†æ•°"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def calculate_trend_score(self, repo: Dict, timeframe: str = 'lifetime') -> float:
        """
        è®¡ç®—é¡¹ç›®è¶‹åŠ¿åˆ†æ•°
        timeframe: 'lifetime', '30days', '7days'
        """
        try:
            created_at = date_parser.parse(repo['created_at'])
            # ç¡®ä¿ä¸¤ä¸ªdatetimeéƒ½æ˜¯naiveç±»å‹
            if created_at.tzinfo is not None:
                created_at = created_at.replace(tzinfo=None)

            stars = repo.get('stargazers_count', 0)
            forks = repo.get('forks_count', 0)

            if timeframe == 'lifetime':
                # åŸæœ‰é€»è¾‘ï¼šåŸºäºé¡¹ç›®æ•´ä¸ªç”Ÿå‘½å‘¨æœŸ
                days_since_creation = max((datetime.now() - created_at).days, 1)
                daily_stars = stars / days_since_creation
                daily_forks = forks / days_since_creation
                trend_score = daily_stars * 0.7 + daily_forks * 0.3

            elif timeframe == '30days':
                # 30å¤©è¶‹åŠ¿ï¼šå‡è®¾æœ€è¿‘30å¤©è·å¾—çš„stars/forksæ¯”ä¾‹æ›´é«˜
                # ä½¿ç”¨æ›´æ¿€è¿›çš„å¢é•¿å‡è®¾æ¥è¯†åˆ«è¿‘æœŸçƒ­é—¨é¡¹ç›®
                days_since_creation = max((datetime.now() - created_at).days, 1)

                # å¯¹äºè¾ƒæ–°çš„é¡¹ç›®ï¼ˆ<30å¤©ï¼‰ï¼Œä½¿ç”¨å®é™…å¤©æ•°
                if days_since_creation <= 30:
                    effective_days = days_since_creation
                else:
                    # å¯¹äºè€é¡¹ç›®ï¼Œå‡è®¾30%çš„starsæ¥è‡ªæœ€è¿‘30å¤©
                    effective_days = 30
                    stars = int(stars * 0.3)  # ä¼°ç®—æœ€è¿‘30å¤©çš„å¢é•¿
                    forks = int(forks * 0.3)

                daily_stars = stars / effective_days
                daily_forks = forks / effective_days
                trend_score = daily_stars * 0.7 + daily_forks * 0.3

            elif timeframe == '7days':
                # 7å¤©è¶‹åŠ¿ï¼šæ›´æ¿€è¿›çš„è¿‘æœŸå¢é•¿ä¼°ç®—
                days_since_creation = max((datetime.now() - created_at).days, 1)

                if days_since_creation <= 7:
                    effective_days = days_since_creation
                else:
                    # å¯¹äºè€é¡¹ç›®ï¼Œå‡è®¾15%çš„starsæ¥è‡ªæœ€è¿‘7å¤©
                    effective_days = 7
                    stars = int(stars * 0.15)
                    forks = int(forks * 0.15)

                daily_stars = stars / effective_days
                daily_forks = forks / effective_days
                trend_score = daily_stars * 0.7 + daily_forks * 0.3

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ—¶é—´æ¡†æ¶: {timeframe}")

            return trend_score
        except Exception as e:
            self.logger.error(f"è®¡ç®—è¶‹åŠ¿åˆ†æ•°å¤±è´¥: {e}")
            return 0.0

    def sort_by_trend_score(self, projects: List[Dict], timeframe: str = 'lifetime') -> List[Dict]:
        """æŒ‰è¶‹åŠ¿åˆ†æ•°æ’åºé¡¹ç›®"""
        for project in projects:
            project['trend_score'] = self.calculate_trend_score(project, timeframe)

        sorted_projects = sorted(projects, key=lambda x: x['trend_score'], reverse=True)
        self.logger.info(f"æŒ‰{timeframe}è¶‹åŠ¿åˆ†æ•°æ’åºäº† {len(sorted_projects)} ä¸ªé¡¹ç›®")
        return sorted_projects


class DiscordNotifier:
    """Discordæ¶ˆæ¯æ¨é€å™¨"""

    def __init__(self, webhook_url: Optional[str] = None, summarizer: Optional['ProjectSummarizer'] = None):
        self.webhook_url = webhook_url or os.getenv('DISCORD_WEBHOOK_URL')
        self.summarizer = summarizer or ProjectSummarizer()
        self.logger = logging.getLogger(__name__)

    def format_project_info(self, repo: Dict, rank: int) -> str:
        """æ ¼å¼åŒ–é¡¹ç›®ä¿¡æ¯ï¼ˆåŒ…å«æ™ºèƒ½æ€»ç»“ï¼‰"""
        name = repo['name']
        stars = repo['stargazers_count']
        forks = repo['forks_count']
        url = repo['html_url']
        language = repo.get('language', 'Unknown')

        # ç”Ÿæˆæ™ºèƒ½æ€»ç»“
        summary = self.summarizer.generate_summary(repo)

        # é™åˆ¶æ€»ç»“é•¿åº¦ï¼Œç¡®ä¿Discordæ¶ˆæ¯ä¸ä¼šå¤ªé•¿
        if len(summary) > 120:
            summary = summary[:120] + '...'

        return f"{rank}. **{name}** - â­{stars:,} ğŸ´{forks:,} ğŸ“{language}\n   ğŸ’¡ {summary}\n   [ğŸ”— æŸ¥çœ‹é¡¹ç›®]({url})"

    def create_discord_embed(self, popular_projects: List[Dict], trending_projects: List[Dict], trend_timeframe: str = 'lifetime') -> Dict:
        """åˆ›å»ºDiscord Embedæ¶ˆæ¯"""
        # æ ¹æ®æ—¶é—´æ¡†æ¶ç”Ÿæˆæ ‡é¢˜å’Œæè¿°
        timeframe_titles = {
            'lifetime': 'æœ€å€¼å¾—å…³æ³¨çš„AIå¼€æºé¡¹ç›®',
            '30days': 'æœ€è¿‘30å¤©ä¸Šå‡æœ€å¿«çš„AIé¡¹ç›®',
            '7days': 'æœ€è¿‘7å¤©ä¸Šå‡æœ€å¿«çš„AIé¡¹ç›®'
        }

        trending_field_names = {
            'lifetime': 'ğŸ“ˆ è¶‹åŠ¿ä¸Šå‡æœ€å¿«çš„AIé¡¹ç›®',
            '30days': 'ğŸ“ˆ æœ€è¿‘30å¤©ä¸Šå‡æœ€å¿«çš„AIé¡¹ç›®',
            '7days': 'ğŸš€ æœ€è¿‘7å¤©ä¸Šå‡æœ€å¿«çš„AIé¡¹ç›®'
        }

        embed = {
            "title": "ğŸ¤– AIé¡¹ç›®æ—¥æŠ¥",
            "description": f"{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')} {timeframe_titles.get(trend_timeframe, timeframe_titles['lifetime'])}",
            "color": 5814783,  # è“è‰²
            "fields": [],
            "timestamp": datetime.now().isoformat()
        }

        # æ·»åŠ çƒ­é—¨é¡¹ç›®
        if popular_projects:
            popular_text = "\n\n".join([
                self.format_project_info(repo, i+1)
                for i, repo in enumerate(popular_projects[:2])
            ])
            embed["fields"].append({
                "name": "â­ æ”¶è—æœ€å¤šçš„AIé¡¹ç›®",
                "value": popular_text,
                "inline": False
            })

        # æ·»åŠ è¶‹åŠ¿é¡¹ç›®
        if trending_projects:
            trending_text = "\n\n".join([
                self.format_project_info(repo, i+1)
                for i, repo in enumerate(trending_projects[:2])
            ])
            embed["fields"].append({
                "name": trending_field_names.get(trend_timeframe, trending_field_names['lifetime']),
                "value": trending_text,
                "inline": False
            })

        return {"embeds": [embed]}

    def send_notification(self, popular_projects: List[Dict], trending_projects: List[Dict], trend_timeframe: str = 'lifetime') -> bool:
        """å‘é€Discordé€šçŸ¥"""
        if not self.webhook_url:
            self.logger.error("Discord Webhook URLæœªé…ç½®")
            return False

        payload = self.create_discord_embed(popular_projects, trending_projects, trend_timeframe)

        try:
            response = requests.post(self.webhook_url, json=payload)
            response.raise_for_status()
            self.logger.info("Discordæ¶ˆæ¯å‘é€æˆåŠŸ")
            return True
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Discordæ¶ˆæ¯å‘é€å¤±è´¥: {e}")
            return False

    def create_commercial_discord_embed(self, popular_projects: List[Dict], trending_projects: List[Dict], trend_timeframe: str = 'lifetime') -> Dict:
        """åˆ›å»ºå•†ç”¨å®ç”¨æ€§AIé¡¹ç›®çš„Discord Embedæ¶ˆæ¯"""
        # æ ¹æ®æ—¶é—´æ¡†æ¶ç”Ÿæˆæ ‡é¢˜å’Œæè¿°
        timeframe_titles = {
            'lifetime': 'æœ€å…·å•†ç”¨ä»·å€¼çš„AIå¼€æºé¡¹ç›®',
            '30days': 'æœ€è¿‘30å¤©å•†ç”¨çƒ­é—¨çš„AIé¡¹ç›®',
            '7days': 'æœ€è¿‘7å¤©å•†ç”¨çˆ†ç«çš„AIé¡¹ç›®'
        }

        trending_field_names = {
            'lifetime': 'ğŸ’¼ å•†ç”¨è¶‹åŠ¿ä¸Šå‡æœ€å¿«çš„AIé¡¹ç›®',
            '30days': 'ğŸ’¼ æœ€è¿‘30å¤©å•†ç”¨çƒ­é—¨çš„AIé¡¹ç›®',
            '7days': 'ğŸš€ æœ€è¿‘7å¤©å•†ç”¨çˆ†ç«çš„AIé¡¹ç›®'
        }

        embed = {
            "title": "ğŸ’¼ å•†ç”¨å®ç”¨æ€§AIé¡¹ç›®æ—¥æŠ¥",
            "description": f"{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')} {timeframe_titles.get(trend_timeframe, timeframe_titles['lifetime'])}",
            "color": 3447003,  # æ·±è“è‰²ï¼Œæ›´å•†åŠ¡æ„Ÿ
            "fields": [],
            "timestamp": datetime.now().isoformat()
        }

        # æ·»åŠ çƒ­é—¨å•†ç”¨é¡¹ç›®
        if popular_projects:
            popular_text = "\n\n".join([
                self.format_project_info(repo, i+1)
                for i, repo in enumerate(popular_projects[:2])
            ])
            embed["fields"].append({
                "name": "ğŸ† æ”¶è—æœ€å¤šçš„å•†ç”¨AIé¡¹ç›®",
                "value": popular_text,
                "inline": False
            })

        # æ·»åŠ å•†ç”¨è¶‹åŠ¿é¡¹ç›®
        if trending_projects:
            trending_text = "\n\n".join([
                self.format_project_info(repo, i+1)
                for i, repo in enumerate(trending_projects[:2])
            ])
            embed["fields"].append({
                "name": trending_field_names.get(trend_timeframe, trending_field_names['lifetime']),
                "value": trending_text,
                "inline": False
            })

        # æ·»åŠ å•†ç”¨ä»·å€¼è¯´æ˜
        embed["fields"].append({
            "name": "ğŸ’¡ å•†ç”¨ä»·å€¼è¯´æ˜",
            "value": "è¿™äº›é¡¹ç›®å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\nâ€¢ ğŸ› ï¸ å³å¼€å³ç”¨çš„å®ç”¨å·¥å…·\nâ€¢ ğŸ’° æ˜ç¡®çš„å•†ä¸šåº”ç”¨åœºæ™¯\nâ€¢ ğŸ”§ å®Œå–„çš„éƒ¨ç½²å’Œé›†æˆæ–¹æ¡ˆ\nâ€¢ ğŸ“ˆ æ´»è·ƒçš„ç¤¾åŒºå’Œç»´æŠ¤å›¢é˜Ÿ",
            "inline": False
        })

        return {"embeds": [embed]}

    def send_commercial_notification(self, popular_projects: List[Dict], trending_projects: List[Dict], trend_timeframe: str = 'lifetime') -> bool:
        """å‘é€å•†ç”¨å®ç”¨æ€§AIé¡¹ç›®Discordé€šçŸ¥"""
        if not self.webhook_url:
            self.logger.error("Discord Webhook URLæœªé…ç½®")
            return False

        payload = self.create_commercial_discord_embed(popular_projects, trending_projects, trend_timeframe)

        try:
            response = requests.post(self.webhook_url, json=payload)
            response.raise_for_status()
            self.logger.info("å•†ç”¨é¡¹ç›®Discordæ¶ˆæ¯å‘é€æˆåŠŸ")
            return True
        except requests.exceptions.RequestException as e:
            self.logger.error(f"å•†ç”¨é¡¹ç›®Discordæ¶ˆæ¯å‘é€å¤±è´¥: {e}")
            return False


class AIGitHubTracker:
    """AI GitHubè¿½è¸ªå™¨ä¸»æ§åˆ¶å™¨"""

    def __init__(self):
        self.setup_logging()
        self.github_client = GitHubAPIClient()
        self.ai_filter = AIProjectFilter()
        self.commercial_filter = CommercialAIProjectFilter()
        self.deduplicator = ProjectDeduplicator()
        self.trend_analyzer = TrendAnalyzer()
        self.summarizer = ProjectSummarizer()
        self.notifier = DiscordNotifier(summarizer=self.summarizer)
        self.logger = logging.getLogger(__name__)

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('ai_tracker.log', encoding='utf-8')
            ]
        )

    def run_daily_tracking(self, trend_timeframe: str = 'lifetime'):
        """
        æ‰§è¡Œæ¯æ—¥è¿½è¸ªä»»åŠ¡
        trend_timeframe: 'lifetime', '30days', '7days'
        """
        self.logger.info(f"å¼€å§‹æ‰§è¡ŒAI GitHubé¡¹ç›®æ¯æ—¥è¿½è¸ªä»»åŠ¡ï¼ˆè¶‹åŠ¿æ—¶é—´æ¡†æ¶: {trend_timeframe}ï¼‰")

        try:
            # 1. æ¸…ç†æ—§è®°å½•
            self.deduplicator.clean_old_records()

            # 2. è·å–é¡¹ç›®æ•°æ®
            self.logger.info("æ­£åœ¨è·å–GitHubé¡¹ç›®æ•°æ®...")
            popular_repos = self.github_client.get_popular_ai_projects()
            trending_repos = self.github_client.get_trending_ai_projects()

            if not popular_repos and not trending_repos:
                self.logger.error("æœªèƒ½è·å–åˆ°ä»»ä½•é¡¹ç›®æ•°æ®")
                return

            # 3. AIé¡¹ç›®è¿‡æ»¤
            popular_ai_projects = self.ai_filter.filter_ai_projects(popular_repos)
            trending_ai_projects = self.ai_filter.filter_ai_projects(trending_repos)

            # 4. å»é‡è¿‡æ»¤
            new_popular_projects = self.deduplicator.filter_new_projects(popular_ai_projects)
            new_trending_projects = self.deduplicator.filter_new_projects(trending_ai_projects)

            # 5. è¶‹åŠ¿åˆ†æï¼ˆä½¿ç”¨æŒ‡å®šçš„æ—¶é—´æ¡†æ¶ï¼‰
            trending_sorted = self.trend_analyzer.sort_by_trend_score(new_trending_projects, trend_timeframe)

            # 6. é€‰æ‹©è¦æ¨é€çš„é¡¹ç›®
            selected_popular = new_popular_projects[:2]  # å‰2ä¸ªçƒ­é—¨é¡¹ç›®
            selected_trending = trending_sorted[:2]      # å‰2ä¸ªè¶‹åŠ¿é¡¹ç›®

            if not selected_popular and not selected_trending:
                self.logger.info("æ²¡æœ‰å‘ç°æ–°çš„AIé¡¹ç›®ï¼Œä»Šæ—¥ä¸æ¨é€")
                return

            # 7. å‘é€é€šçŸ¥ï¼ˆä¼ é€’æ—¶é—´æ¡†æ¶ä¿¡æ¯ï¼‰
            discord_success = self.notifier.send_notification(selected_popular, selected_trending, trend_timeframe)

            # 8. æ ‡è®°é¡¹ç›®ä¸ºå·²æ¨é€ï¼ˆæ— è®ºDiscordæ˜¯å¦æˆåŠŸï¼‰
            for project in selected_popular + selected_trending:
                self.deduplicator.mark_project_as_sent(project)

            if discord_success:
                self.logger.info(f"âœ… æˆåŠŸæ¨é€ {len(selected_popular)} ä¸ªçƒ­é—¨é¡¹ç›®å’Œ {len(selected_trending)} ä¸ªè¶‹åŠ¿é¡¹ç›®")
            else:
                self.logger.warning(f"âš ï¸ Discordæ¶ˆæ¯å‘é€å¤±è´¥ï¼Œä½†å·²è®°å½• {len(selected_popular)} ä¸ªçƒ­é—¨é¡¹ç›®å’Œ {len(selected_trending)} ä¸ªè¶‹åŠ¿é¡¹ç›®")

        except Exception as e:
            self.logger.error(f"æ‰§è¡Œè¿½è¸ªä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise

    def run_multi_timeframe_tracking(self):
        """æ‰§è¡Œå¤šæ—¶é—´æ¡†æ¶è¿½è¸ªï¼Œåˆ†åˆ«æ¨é€30å¤©å’Œ7å¤©è¶‹åŠ¿"""
        timeframes = [
            ('30days', 'ğŸ“ˆ æœ€è¿‘30å¤©ä¸Šå‡æœ€å¿«çš„AIé¡¹ç›®'),
            ('7days', 'ğŸš€ æœ€è¿‘7å¤©ä¸Šå‡æœ€å¿«çš„AIé¡¹ç›®')
        ]

        for timeframe, description in timeframes:
            self.logger.info(f"æ‰§è¡Œ{description}...")
            try:
                # è·å–æ•°æ®
                popular_repos = self.github_client.get_popular_ai_projects()
                trending_repos = self.github_client.get_trending_ai_projects()

                if not trending_repos:
                    continue

                # è¿‡æ»¤å’Œå»é‡
                trending_ai_projects = self.ai_filter.filter_ai_projects(trending_repos)
                new_trending_projects = self.deduplicator.filter_new_projects(trending_ai_projects)

                # æŒ‰æŒ‡å®šæ—¶é—´æ¡†æ¶æ’åº
                trending_sorted = self.trend_analyzer.sort_by_trend_score(new_trending_projects, timeframe)
                selected_trending = trending_sorted[:2]

                if selected_trending:
                    # å‘é€é€šçŸ¥
                    if self.notifier.send_notification([], selected_trending, timeframe):
                        for project in selected_trending:
                            self.deduplicator.mark_project_as_sent(project)
                        self.logger.info(f"æˆåŠŸæ¨é€{timeframe}è¶‹åŠ¿é¡¹ç›®")

            except Exception as e:
                self.logger.error(f"æ‰§è¡Œ{timeframe}è¿½è¸ªæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue

    def run_commercial_tracking(self, trend_timeframe: str = 'lifetime'):
        """
        æ‰§è¡Œå•†ç”¨å®ç”¨æ€§AIé¡¹ç›®è¿½è¸ªä»»åŠ¡
        trend_timeframe: 'lifetime', '30days', '7days'
        """
        self.logger.info(f"å¼€å§‹æ‰§è¡Œå•†ç”¨å®ç”¨æ€§AIé¡¹ç›®è¿½è¸ªä»»åŠ¡ï¼ˆè¶‹åŠ¿æ—¶é—´æ¡†æ¶: {trend_timeframe}ï¼‰")
        try:
            # 1. æ¸…ç†æ—§è®°å½•
            self.deduplicator.clean_old_records()

            # 2. è·å–é¡¹ç›®æ•°æ®
            self.logger.info("æ­£åœ¨è·å–GitHubé¡¹ç›®æ•°æ®...")
            popular_repos = self.github_client.get_popular_ai_projects()
            trending_repos = self.github_client.get_trending_ai_projects()

            if not popular_repos and not trending_repos:
                self.logger.error("æœªèƒ½è·å–åˆ°ä»»ä½•é¡¹ç›®æ•°æ®")
                return

            # 3. å•†ç”¨å®ç”¨æ€§AIé¡¹ç›®è¿‡æ»¤
            popular_commercial_projects = self.commercial_filter.filter_commercial_ai_projects(popular_repos)
            trending_commercial_projects = self.commercial_filter.filter_commercial_ai_projects(trending_repos)

            # 4. å»é‡è¿‡æ»¤
            new_popular_projects = self.deduplicator.filter_new_projects(popular_commercial_projects)
            new_trending_projects = self.deduplicator.filter_new_projects(trending_commercial_projects)

            # 5. è¶‹åŠ¿åˆ†æï¼ˆä½¿ç”¨æŒ‡å®šçš„æ—¶é—´æ¡†æ¶ï¼‰
            trending_sorted = self.trend_analyzer.sort_by_trend_score(new_trending_projects, trend_timeframe)

            # 6. é€‰æ‹©è¦æ¨é€çš„é¡¹ç›®
            selected_popular = new_popular_projects[:2]  # å‰2ä¸ªçƒ­é—¨å•†ç”¨é¡¹ç›®
            selected_trending = trending_sorted[:2]      # å‰2ä¸ªå•†ç”¨è¶‹åŠ¿é¡¹ç›®

            if not selected_popular and not selected_trending:
                self.logger.info("æ²¡æœ‰å‘ç°æ–°çš„å•†ç”¨å®ç”¨æ€§AIé¡¹ç›®ï¼Œä»Šæ—¥ä¸æ¨é€")
                return

            # 7. å‘é€é€šçŸ¥ï¼ˆä½¿ç”¨å•†ç”¨é¡¹ç›®ä¸“ç”¨æ ¼å¼ï¼‰
            discord_success = self.notifier.send_commercial_notification(selected_popular, selected_trending, trend_timeframe)

            # 8. æ ‡è®°é¡¹ç›®ä¸ºå·²æ¨é€ï¼ˆæ— è®ºDiscordæ˜¯å¦æˆåŠŸï¼‰
            for project in selected_popular + selected_trending:
                self.deduplicator.mark_project_as_sent(project)

            if discord_success:
                self.logger.info(f"âœ… æˆåŠŸæ¨é€ {len(selected_popular)} ä¸ªçƒ­é—¨å•†ç”¨é¡¹ç›®å’Œ {len(selected_trending)} ä¸ªå•†ç”¨è¶‹åŠ¿é¡¹ç›®")
            else:
                self.logger.warning(f"âš ï¸ Discordæ¶ˆæ¯å‘é€å¤±è´¥ï¼Œä½†å·²è®°å½• {len(selected_popular)} ä¸ªçƒ­é—¨å•†ç”¨é¡¹ç›®å’Œ {len(selected_trending)} ä¸ªå•†ç”¨è¶‹åŠ¿é¡¹ç›®")

        except Exception as e:
            self.logger.error(f"æ‰§è¡Œå•†ç”¨é¡¹ç›®è¿½è¸ªä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='AI GitHub Daily Tracker - AIé¡¹ç›®æ¯æ—¥è¿½è¸ªå™¨')
    parser.add_argument('--reset', action='store_true',
                       help='é‡ç½®å·²æ¨é€é¡¹ç›®è®°å½•ï¼Œä¸‹æ¬¡å°†æ¨é€æœ€çƒ­é—¨çš„é¡¹ç›®')
    parser.add_argument('--stats', action='store_true',
                       help='æ˜¾ç¤ºæ¨é€ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--trend-timeframe', choices=['lifetime', '30days', '7days'],
                       default='lifetime',
                       help='è¶‹åŠ¿åˆ†ææ—¶é—´æ¡†æ¶ (é»˜è®¤: lifetime)')
    parser.add_argument('--multi-timeframe', action='store_true',
                       help='æ‰§è¡Œå¤šæ—¶é—´æ¡†æ¶è¿½è¸ªï¼ˆ30å¤©å’Œ7å¤©è¶‹åŠ¿ï¼‰')
    parser.add_argument('--commercial', action='store_true',
                       help='æ‰§è¡Œå•†ç”¨å®ç”¨æ€§AIé¡¹ç›®è¿½è¸ª')

    args = parser.parse_args()

    tracker = AIGitHubTracker()

    if args.reset:
        print("ğŸ”„ é‡ç½®å·²æ¨é€é¡¹ç›®è®°å½•...")
        tracker.deduplicator.reset_sent_projects()
        print("âœ… é‡ç½®å®Œæˆï¼ä¸‹æ¬¡è¿è¡Œå°†æ¨é€æœ€çƒ­é—¨çš„AIé¡¹ç›®ã€‚")
        return

    if args.stats:
        stats = tracker.deduplicator.get_stats()
        print("ğŸ“Š æ¨é€ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  å·²æ¨é€é¡¹ç›®æ€»æ•°: {stats['total_sent']}")
        print(f"  æœ€åæ¨é€æ—¶é—´: {stats['latest_sent'] or 'N/A'}")
        print(f"  å­˜å‚¨æ–‡ä»¶: {stats['storage_file']}")
        return

    if args.commercial:
        # æ‰§è¡Œå•†ç”¨å®ç”¨æ€§AIé¡¹ç›®è¿½è¸ª
        tracker.run_commercial_tracking(args.trend_timeframe)
    elif args.multi_timeframe:
        # æ‰§è¡Œå¤šæ—¶é—´æ¡†æ¶è¿½è¸ª
        tracker.run_multi_timeframe_tracking()
    else:
        # æ‰§è¡Œå¸¸è§„è¿½è¸ªï¼Œä½¿ç”¨æŒ‡å®šçš„æ—¶é—´æ¡†æ¶
        tracker.run_daily_tracking(args.trend_timeframe)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        import traceback
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print("é”™è¯¯è¯¦æƒ…:")
        traceback.print_exc()
        exit(1)